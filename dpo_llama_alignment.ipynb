{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7142ab-f091-4092-936a-66d719c7edcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "[2024-12-10 03:25:22,490] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "#\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from evaluate import load\n",
    "\n",
    "# DPO stuff\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from unsloth import PatchDPOTrainer\n",
    "PatchDPOTrainer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff6a9ed-8db6-415d-b3f3-221c4d8b9f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merkara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/wandb/run-20241210_032525-v0ciwv7n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/erkara/dpo_Honaz/runs/v0ciwv7n' target=\"_blank\">upbeat-pond-8</a></strong> to <a href='https://wandb.ai/erkara/dpo_Honaz' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/erkara/dpo_Honaz' target=\"_blank\">https://wandb.ai/erkara/dpo_Honaz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/erkara/dpo_Honaz/runs/v0ciwv7n' target=\"_blank\">https://wandb.ai/erkara/dpo_Honaz/runs/v0ciwv7n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/erkara/dpo_Honaz/runs/v0ciwv7n?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x77947006a050>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Load environment variables and configure device.\"\"\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(f\"CUDA not found\")\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(\"all_keys.txt\")\n",
    "\n",
    "# Register HuggingFace --> replace your key\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# initizalie wandb with gradient info as well.\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "wandb.login(key=wandb_api_key)\n",
    "wandb.init(project=\"dpo_Honaz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e791bc4-6498-432d-aa6c-ead2c58d2da8",
   "metadata": {},
   "source": [
    "# Aligning LLMs with Direct Preference Optimization(DPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa611d-4e96-4f83-a142-62e7459c0953",
   "metadata": {},
   "source": [
    "This project builds on a previous effort to fine-tune a language model using a niche dataset about my home town [Honaz](https://en.wikipedia.org/wiki/Honaz), a small town in Turkey. The dataset was created from three detailed Turkish articles sourced from the [DergiPark](https://dergipark.org.tr/) repo. We created our own instruction dataset and fine-tune `Llama-3.2-1B-Instruct` on it. \n",
    "\n",
    "Now, we aim to take the fine-tuning process further by applying Direct Preference Optimization (DPO). The objective is to align the model's responses not just to be accurate but also to reflect a conversational, informal tone that resonates with the target audience. Target audience here is my dad since he does not like to read serious stuff, gets bored quickly.(I can translate stuff to Turkish). Just like before, we created our alignment data, which I will outline in a seperate entry in this repo. The focus will be on generating and refining pairwise comparison data, where the preferred responses align with this style, ensuring the model is not only knowledgeable but also user-friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c6972-b40f-4868-9140-1eff286eff76",
   "metadata": {},
   "source": [
    "## Preference Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b2213f-8b61-49f1-a13c-5c5578c84b83",
   "metadata": {},
   "source": [
    "Lets load and see how our data looks like. As you see, the accepted answers has pretty informal tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0edd24a-b756-48ac-96b0-86357bf07cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d99d2a-8b57-42eb-a795-715cce770c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"erdi28/alignment-dataset-honaz\",split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97414a57-530c-40a1-978e-c6c76985575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer a question based on the following content.\n",
      "\n",
      "3. VEGETATION OF HONAZ MOUNTAIN AND ITS SURROUNDINGS\n",
      "The vegetation of Honaz Mountain and its surroundings generally consists of dry forests dominated by red pines at lower elevations and black pines at higher elevations. The northern slopes of the Honaz massif are influenced by the Mediterranean climate that penetrates along the BÃ¼yÃ¼k Menderes valley, while the interior areas and southern slopes are under the influence of a continental climate. As a result, the vegetation on the northern and southern slopes of the massif differs. On the more humid northern slopes, a richer and more diverse maquis formation has developed, whereas on the southern slopes, a garigue formation consisting of only the most drought-resistant maquis species is prevalent.\n",
      "=====================================================\n",
      "The vegetation of Honaz Mountain and its surroundings is characterized by dry forests, with red pines at lower elevations and black pines at higher elevations. The northern slopes experience a Mediterranean climate, fostering a diverse maquis formation, while the southern slopes, influenced by a continental climate, predominantly feature a garigue formation composed of drought-resistant maquis species.\n",
      "=====================================================\n",
      "So, Honaz Mountain has some pretty cool vegetation vibes going on! Down at the lower elevations, you've got these dry forests mostly filled with red pines, and as you go up higher, you switch to black pines. The northern slopes are more influenced by the Mediterranean climate, which makes them all lush and diverse with a rich maquis formation. But then, head over to the southern slopes, and it's a whole different sceneâ€”it's drier there, so you get this garigue formation with only the toughest, drought-resistant plants. It's like two different worlds on the same mountain!\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0]['prompt'])\n",
    "print(\"=====================================================\")\n",
    "print(dataset[0]['rejected'])\n",
    "print(\"=====================================================\")\n",
    "print(dataset[0]['chosen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b5955-2f92-40be-9d41-5830e1b21baa",
   "metadata": {},
   "source": [
    "# Alignmet with DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a190e6-26b6-4645-b795-df642b8bf483",
   "metadata": {},
   "source": [
    "First load the fine-tuned model and inspect how it generates its answer. We can observe that that is pretty text-book voice and formal answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b17232-5e20-4c9e-aa06-1063d3fecda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.381 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.4 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "ref_model, tokenizer = FastLanguageModel.from_pretrained(model_name = \"erdi28/finetune_llama_honaz\",\n",
    "                                                     max_seq_length = max_seq_length,\n",
    "                                                     dtype = None,                         \n",
    "                                                     load_in_4bit = True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c934beb-01aa-4833-a9b4-5336a8e46cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Alpaca prompt template ( we dont have \"Input\" field)\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2565a7f6-ed2b-4677-b035-47aa44240d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the climatic influences on Honaz Mountainâ€™s vegetation?\n",
      "\n",
      "### Response:\n",
      "Honaz Mountain, located in the Aegean region of Turkey, experiences a Mediterranean climate characterized by mild winters and warm summers. This climate influences the vegetation types present in the area. \n",
      "\n",
      "1. **Temperature and Precipitation Patterns**: The mountain experiences a significant variation in temperature between summer and winter. Summer temperatures can reach up to 32Â°C (90Â°F) during the peak summer months, while winters can drop to around 2Â°C (36Â°F). Precipitation on Honaz Mountain is generally well-distributed, with most of the annual rainfall falling during the winter months.\n",
      "\n",
      "2. **Rainfall Distribution**: The annual rainfall on Honaz Mountain is substantial, with the majority of it occurring between October and April. This rainfall is crucial\n"
     ]
    }
   ],
   "source": [
    "def generate_streaming_text(model, tokenizer, prompt, max_new_tokens=256, prompt_template=alpaca_prompt):\n",
    "    \"\"\"\n",
    "    Generates text from a model with streaming output.\n",
    "    \"\"\"\n",
    "    # format the input and set up the stremaer\n",
    "    message = prompt_template.format(prompt, \"\")\n",
    "    inputs = tokenizer([message], return_tensors=\"pt\").to(device)\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "    \n",
    "    # Generate text with streaming\n",
    "    _ = model.generate(\n",
    "        **inputs, \n",
    "        streamer=text_streamer, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "# test\n",
    "ref_model = FastLanguageModel.for_inference(ref_model)\n",
    "prompt = \"What are the climatic influences on Honaz Mountainâ€™s vegetation?\"\n",
    "generate_streaming_text(ref_model, tokenizer, prompt, max_new_tokens=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b572c4-9686-4c95-9383-74d6ce0ad9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5922383b-d5e3-49b0-bdc4-2e865a512e4c",
   "metadata": {},
   "source": [
    "Now lets go ahead and configure LORA paramater. *At the time of this notebook, there was an ongoing error that despite the fact that we saved the full model to hub, Unsloth does not stop tracking LORA paramaters causing us to use exactly the same LORA configuration we use to fine-tune the original model. It is a major problem but we can live with it for now* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c55ef4e-083a-4e61-a186-6696e303b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    ref_model,\n",
    "    r = 32,            \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0,         # dropout after adapter, \"0\" is optimized\n",
    "    bias = \"none\",            # biases in the model remain frozen (not updated), \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    use_rslora = False,     # rank stabilized LoRA\n",
    "    loftq_config = None,    # And LoftQ\n",
    "    random_state = 1234,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e9f2b9-a701-4865-9edb-6f19f6b76261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Alpaca prompt template\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. \n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Ensure the EOS token is defined\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Mapping function to format the dataset\n",
    "def format_samples(example):\n",
    "    example[\"prompt\"] = alpaca_prompt.format(example[\"prompt\"])\n",
    "    example[\"chosen\"] = example[\"chosen\"] + EOS_TOKEN\n",
    "    example[\"rejected\"] = example[\"rejected\"] + EOS_TOKEN\n",
    "\n",
    "    return {\n",
    "        \"prompt\": example[\"prompt\"],\n",
    "        \"chosen\": example[\"chosen\"],\n",
    "        \"rejected\": example[\"rejected\"],\n",
    "    }\n",
    "\n",
    "# Apply the mapping function to the dataset\n",
    "dataset = dataset.map(format_samples)\n",
    "dataset = dataset.train_test_split(test_size=0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b72b431-7587-4ca5-b745-7ba70c35b8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"].num_rows)\n",
    "print(dataset[\"test\"].num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49926005-a799-4262-a225-14a92c30fcb6",
   "metadata": {},
   "source": [
    "Here is our driver code. I cannot give a full lecture on how DPO works but here are few key points:\n",
    "- We use a smaller learning rate 3e-6 as opposed to 3e-4 in finetuning. `beta` paramaters controls balance between the model's pre-trained distribution and the preference-aligned distribution. While low beta strongly aligns with base model, high beta puts more emphasis on matching the reward signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6777e1b-9b59-4999-87a7-b3d75de85f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DPOTrainer(\n",
    "    model= model,\n",
    "    ref_model= None,\n",
    "    tokenizer = tokenizer,\n",
    "    beta = .8,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"test\"],\n",
    "    max_length = max_seq_length//2,\n",
    "    max_prompt_length = max_seq_length//2,\n",
    "    dataset_num_proc = 2,                 # number of parallel proceses for data preprocessing\n",
    "    args = DPOConfig(\n",
    "         # Training hyperparameters\n",
    "        num_train_epochs=1,               # Train for one epoch\n",
    "        per_device_train_batch_size=2,    # Batch size per device during training\n",
    "        per_device_eval_batch_size=2,     # Batch size per device during evaluation\n",
    "        gradient_accumulation_steps=8,    # Accumulate gradients for larger effective batch size\n",
    "        gradient_checkpointing=True,      # Save memory by recomputing activations in backprop\n",
    "        \n",
    "        # Optimization settings\n",
    "        learning_rate = 2e-5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01, \n",
    "        lr_scheduler_type = \"linear\",      \n",
    "        warmup_steps=10,\n",
    "        \n",
    "        # Precision settings\n",
    "        fp16 = not is_bfloat16_supported(),     # Disable FP16 precision (set True if supported)\n",
    "        bf16 = is_bfloat16_supported(),         # Disable BF16 precision (use True on A100 GPUs)\n",
    "        \n",
    "       # Logging and checkpoints\n",
    "        save_steps=100,                   # Save checkpoint every 100 steps\n",
    "        save_total_limit=1,               # Keep only the most recent checkpoint\n",
    "        logging_steps=10,                  # Log training progress every 25 steps\n",
    "        eval_strategy=\"steps\",            # Run evaluation at regular intervals, dont wait epochs\n",
    "        eval_steps=10,                     # Evaluate in every such steps\n",
    "        output_dir= \"output_dpo\",\n",
    "        run_name=\"llama_dpo\",\n",
    "        report_to=\"wandb\",                # Report metrics to Weights and Biases\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fb87b62-b508-41fd-8f9b-647ae5140455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 909 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 16 | Total steps = 56\n",
      " \"-____-\"     Number of trainable parameters = 22,544,384\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 02:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>rewards / chosen</th>\n",
       "      <th>rewards / rejected</th>\n",
       "      <th>rewards / accuracies</th>\n",
       "      <th>rewards / margins</th>\n",
       "      <th>logps / rejected</th>\n",
       "      <th>logps / chosen</th>\n",
       "      <th>logits / rejected</th>\n",
       "      <th>logits / chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9.264900</td>\n",
       "      <td>2.473232</td>\n",
       "      <td>30.319460</td>\n",
       "      <td>23.740694</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>6.578768</td>\n",
       "      <td>-115.043007</td>\n",
       "      <td>-238.793442</td>\n",
       "      <td>4.147682</td>\n",
       "      <td>4.467786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.991600</td>\n",
       "      <td>0.446189</td>\n",
       "      <td>38.661587</td>\n",
       "      <td>17.798258</td>\n",
       "      <td>0.931373</td>\n",
       "      <td>20.863325</td>\n",
       "      <td>-122.471039</td>\n",
       "      <td>-228.365829</td>\n",
       "      <td>4.438614</td>\n",
       "      <td>4.441101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.232100</td>\n",
       "      <td>0.221486</td>\n",
       "      <td>40.632534</td>\n",
       "      <td>12.861235</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>27.771303</td>\n",
       "      <td>-128.642334</td>\n",
       "      <td>-225.902130</td>\n",
       "      <td>4.517467</td>\n",
       "      <td>4.417289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.185400</td>\n",
       "      <td>0.144026</td>\n",
       "      <td>41.185730</td>\n",
       "      <td>10.820862</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>30.364866</td>\n",
       "      <td>-131.192810</td>\n",
       "      <td>-225.210648</td>\n",
       "      <td>4.548862</td>\n",
       "      <td>4.415908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.140400</td>\n",
       "      <td>0.134067</td>\n",
       "      <td>41.379635</td>\n",
       "      <td>10.155040</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>31.224592</td>\n",
       "      <td>-132.025070</td>\n",
       "      <td>-224.968277</td>\n",
       "      <td>4.552331</td>\n",
       "      <td>4.411156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=56, training_loss=2.137345226747649, metrics={'train_runtime': 146.6748, 'train_samples_per_second': 6.197, 'train_steps_per_second': 0.382, 'total_flos': 0.0, 'train_loss': 2.137345226747649, 'epoch': 0.9846153846153847})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca64c0-0a50-4892-9d3f-66d5461efed0",
   "metadata": {},
   "source": [
    "`What are those numbers above?`\n",
    "\n",
    "1. `Training\\Validation Loss`: Average loss on the training\\validation dataset. \n",
    "\n",
    "2. `rewards / chosen`: The average reward for the preferred (chosen) responses. Higher values indicate alignment with the reward model.\n",
    "\n",
    "3. `rewards / rejected`: The average reward for the rejected responses. Ideally, this should be lower than `rewards / chosen`.\n",
    "\n",
    "4. `rewards / accuracies`: The fraction of examples where the preferred response has a higher reward than the rejected one. High values (>0.9) indicate strong preference alignment. We gotto watch out this, ideally we like around >.90 without overfitting of course\n",
    "\n",
    "5. `rewards / margins`:The difference between rewards of chosen and rejected responses (`rewards / chosen - rewards / rejected`). Larger margins indicate confident preference alignment.\n",
    "\n",
    "6. `logps / chosen`:Log probability assigned to the chosen responses. Higher values (less negative) indicate the model's confidence in preferred responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702aa4e4-dd75-4dae-942d-1fabbc694d27",
   "metadata": {},
   "source": [
    "While training with DPO, keep an eye on a few key metrics to make sure the model is learning to align with your preference dataset. First, check that `rewards / chosen` is higher than `rewards / rejected`, meaning the preferred responses are actually being rewarded more. If `rewards / accuracies` is above 0.9, thatâ€™s a good sign the model is picking the preferred responses most of the time. Also, watch `rewards / margins`â€”a positive and growing margin shows the model is confidently separating the chosen and rejected responses. For log probabilities, `logps / chosen` should be higher (less negative) than `logps / rejected`, so the model is favoring the better responses. Of course, at the end of the day, we may go back add more data to our alignant dataset as we currently have only 1K pairs. Since this is a personal project and no one is paying for us yet(:, we can call it success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "489ce882-4a05-4fce-bb4f-66daae069182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 152.47 out of 216.26 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 96.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0d99c215f54811a409a50b91eeb6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/595 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7b5e1dcadf415397f7724f77b9f99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/90.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/erdi28/dpo_llama_honaz\n"
     ]
    }
   ],
   "source": [
    "#lets send the model to hub\n",
    "# Save the whole model and push to HuggingFace for further usage\n",
    "model.save_pretrained_merged(\"dpo_llama_honaz\", tokenizer,save_method=\"merged_16bit\")\n",
    "model.push_to_hub(\"erdi28/dpo_llama_honaz\", tokenizer,save_method=\"merged_16bit\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a06924-6ade-4e3d-9f5f-7afdb48bbff5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd80ea5a-4609-4f23-94bb-8432103f6546",
   "metadata": {},
   "source": [
    "Lets look at some response before and after alignment. One can definetely the improvement in the direction of more informal response with more room for improvement of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c24866e-fa9a-4d57-83c8-8f3e43e7eda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are major tourist destination places in Honaz?\n",
      "\n",
      "### Response:\n",
      "Honaz is a charming town in Turkey, known for its beautiful beaches, lush green valleys, and rich history. Here are some major tourist destination places in Honaz:\n",
      "\n",
      "1. **Beaches**: Honaz is famous for its stunning beaches, particularly the KireniÅŸli Beach, which is known for its white sand and crystal-clear waters. The town also boasts other beautiful beaches, such as GÃ¼zelpÄ±nar Beach and GÃ¼zelpÄ±nar Reef.\n",
      "\n",
      "2. **DÃ¶ker Tepe**: This hill is a popular spot for panoramic views of the town and the surrounding landscape. Visitors can reach the summit by hiking up the steep trails or by taking the nearby DÃ¶ker Train.\n",
      "\n",
      "3. **HÃ¶yÃ¼k Beach**: This\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.for_inference(model)\n",
    "prompt = \"What are major tourist destination places in Honaz?\"\n",
    "generate_streaming_text(model, tokenizer, prompt, max_new_tokens=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acb5a3ee-41e6-43f4-a102-f4f115ece99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.381 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "#load rhe base model agaib\n",
    "max_seq_length = 2048\n",
    "ref_model, tokenizer = FastLanguageModel.from_pretrained(model_name = \"erdi28/finetune_llama_honaz\",\n",
    "                                                     max_seq_length = max_seq_length,\n",
    "                                                     dtype = None,                         \n",
    "                                                     load_in_4bit = True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2346ec03-2356-4741-9985-5cb150ccb5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are major tourist destination places in Honaz?\n",
      "\n",
      "### Response:\n",
      "Honaz is a popular tourist destination in Turkey, known for its stunning natural beauty, rich history, and unique cultural heritage. Some of the major tourist destination places in Honaz include:\n",
      "\n",
      "1. Honaz Beach: Located on the southern coast of Turkey, the beach is known for its white sand and crystal-clear waters, making it a perfect spot for swimming, sunbathing, and relaxation.\n",
      "\n",
      "2. Kastro Village: This historic village is one of the oldest in Western Anatolia and features traditional Ottoman architecture, beautiful gardens, and a fascinating museum showcasing the region's history and culture.\n",
      "\n",
      "3. Tursunlu Cliff: A breathtaking natural formation, the Tursunlu Cliff is a stunning sight to behold, especially during sunrise when the sun\n"
     ]
    }
   ],
   "source": [
    "ref_model = FastLanguageModel.for_inference(ref_model)\n",
    "prompt = \"What are major tourist destination places in Honaz?\"\n",
    "generate_streaming_text(ref_model, tokenizer, prompt, max_new_tokens=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a32398-b949-4405-8d59-7d15c055a282",
   "metadata": {},
   "source": [
    "**FINAL NOTE**: Note that we `monitor` our tranining progress in wandb dashboard, we should actually do it not just let it sit there. For example, in additoon to usual stuff like validaton loss, pay attention to grad norms there to make sure the tranining process is actually stable. If not, turn on stuff like `gradient clipping` in the Trainer above. For now, that's all, it is 10:30pm, little Alfie is crying, I need to take care of him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcfc18-46e3-45aa-b8fe-ea299831d4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
